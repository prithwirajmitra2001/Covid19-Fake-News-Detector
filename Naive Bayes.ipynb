{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>target(fake=0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good news covafirst vaccine get approval human...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>country first indigenous corona vaccine covade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>india first corona vaccine candidate cova set ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anildeshmukhncp pypayurved bought untested hom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mohap announces new corona case recovery</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  target(fake=0)\n",
       "0  good news covafirst vaccine get approval human...               1\n",
       "1  country first indigenous corona vaccine covade...               1\n",
       "2  india first corona vaccine candidate cova set ...               1\n",
       "3  anildeshmukhncp pypayurved bought untested hom...               1\n",
       "4           mohap announces new corona case recovery               1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'Clean Data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        \n",
    "    def clean(self):\n",
    "        \n",
    "        for i in range(len(self.df)):\n",
    "\n",
    "            text = self.df.loc[i,\"Tweets\"]\n",
    "            \n",
    "            # Removing special syntax\n",
    "            text = re.sub(r\"(b')+\" , \"\" , text)\n",
    "\n",
    "            # Removing URls\n",
    "            text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(https?://[^\\s]+))' , \"\" , text)\n",
    "            text = re.sub(r'http\\S+' , \"\" , text)\n",
    "\n",
    "            # Removing Usernames\n",
    "            text = re.sub(r'@[^\\s]+' , \"\" , text)\n",
    "\n",
    "            # Removing Hashtags\n",
    "            text = re.sub(r'#([^\\s]+)' , r'\\1' , text)\n",
    "\n",
    "            # Removing HTML Tags\n",
    "            text = re.sub(r'<.*?>' , \"\" , text)\n",
    "\n",
    "            # Removing Emogis\n",
    "            emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "            text = emoji_pattern.sub(r'', text) \n",
    "\n",
    "            # Removing special Emogis\n",
    "            text = (text.\n",
    "                    replace('\\\\xe2\\\\x80\\\\x99', \"'\").\n",
    "                    replace('\\\\xc3\\\\xa9', 'e').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x90', '-').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x91', '-').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x92', '-').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x93', '-').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x98', \"'\").\n",
    "                    replace('\\\\xe2\\\\x80\\\\x9b', \"'\").\n",
    "                    replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x9d', '\"').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x9e', '\"').\n",
    "                    replace('\\\\xe2\\\\x80\\\\x9f', '\"').\n",
    "                    replace('\\\\xe2\\\\x80\\\\xa6', '...').\n",
    "                    replace('\\\\xe2\\\\x80\\\\xb2', \"'\").\n",
    "                    replace('\\\\xe2\\\\x80\\\\xb3', \"'\").\n",
    "                    replace('\\\\xe2\\\\x80\\\\xb4', \"'\").\n",
    "                    replace('\\\\xe2\\\\x80\\\\xb5', \"'\").\n",
    "                    replace('\\\\xe2\\\\x80\\\\xb6', \"'\").\n",
    "                    replace('\\\\xe2\\\\x80\\\\xb7', \"'\").\n",
    "                    replace('\\\\xe2\\\\x81\\\\xba', \"+\").\n",
    "                    replace('\\\\xe2\\\\x81\\\\xbb', \"-\").\n",
    "                    replace('\\\\xe2\\\\x81\\\\xbc', \"=\").\n",
    "                    replace('\\\\xe2\\\\x81\\\\xbd', \"(\").\n",
    "                    replace('\\\\xe2\\\\x81\\\\xbe', \")\")\n",
    "                         )    \n",
    "\n",
    "            # Lower and stopwords removal\n",
    "            text = \" \".join([word for word in text.lower().split() if not word in set(stopwords.words('english'))])\n",
    "\n",
    "            # Punctuation Removal\n",
    "            text = \"\".join([char if char not in string.punctuation else ' ' for char in text])\n",
    "\n",
    "            # Number Removal\n",
    "            text = re.sub(r\"[^A-Z a-z]\" , \"\" , text)\n",
    "\n",
    "            # Emogi Residual Removal\n",
    "            text = re.sub(r\"x[a-z]+ \" , \"\" , text)\n",
    "\n",
    "            # Removing words or length less than 2\n",
    "            text = \" \".join([word for word in text.split() if len(word)>2])\n",
    "\n",
    "            # Removing double or trailing spaces\n",
    "            text = \" \".join(text.split())\n",
    "            \n",
    "            # Similar word removal\n",
    "            corona_similar = ['novelcoronavirus' , 'covid19' , 'covid' , 'corona' , 'coronavirus']\n",
    "            india_similar = ['indian']\n",
    "            for word in corona_similar:\n",
    "                text = re.sub(word , \"corona\" , text)\n",
    "            for word in india_similar:\n",
    "                text = re.sub(word , \"india\" , text)\n",
    "\n",
    "            # Lemmatize\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "            \n",
    "            self.df.loc[i,\"Tweets\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Count:\n",
    "    \n",
    "    def __init__(self , data):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.true_count = []\n",
    "        self.false_count = []\n",
    "        self.true_num = 0\n",
    "        self.false_num = 0\n",
    "        \n",
    "    def count(self, prob):\n",
    "                \n",
    "        if (prob == \"word\"):\n",
    "            \n",
    "            ##  ----------------------------  TRUE COUNT  ----------------------------  ##\n",
    "            \n",
    "            unique = []\n",
    "            \n",
    "            df = self.data[self.data['target(fake=0)']==1].reset_index(drop=True)\n",
    "    \n",
    "            for i in range(len(df)):\n",
    "                for word in df.loc[i,\"Tweets\"].split(\", \"):\n",
    "                    if word not in unique:\n",
    "                        unique.append(word)\n",
    "            self.true_count = dict(zip(unique , np.zeros(len(unique))))\n",
    "            \n",
    "            for i in range(len(df)):\n",
    "                for word in df.loc[i,\"Tweets\"].split(\", \"):\n",
    "                    self.true_count[word] += 1            \n",
    "            \n",
    "            for word in self.true_count.keys():\n",
    "                self.true_num += self.true_count[word]\n",
    "                \n",
    "            ##  ----------------------------  FALSE COUNT  ----------------------------  ##\n",
    "            \n",
    "            unique = []\n",
    "            \n",
    "            df = self.data[self.data['target(fake=0)']==0].reset_index(drop=True)\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                for word in df.loc[i,\"Tweets\"].split(\", \"):\n",
    "                    if word not in unique:\n",
    "                        unique.append(word)\n",
    "                        \n",
    "            self.false_count = dict(zip(unique , np.zeros(len(unique))))\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                for word in df.loc[i,\"Tweets\"].split(\", \"):\n",
    "                    self.false_count[word] += 1            \n",
    "\n",
    "            for word in self.false_count.keys():\n",
    "                self.false_num += self.false_count[word]\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            ##  ----------------------------  TRUE COUNT  ----------------------------  ##\n",
    "            \n",
    "            unique_word = []\n",
    "            \n",
    "            df = self.data[self.data['target(fake=0)']==1].reset_index(drop=True)\n",
    "            \n",
    "            for i in range(len(df)):\n",
    "                unique = []\n",
    "                text = df.loc[i,\"Tweets\"].split(\", \")\n",
    "                for word in text:\n",
    "                    if (text.count(word)>1 and (word not in unique) or text.count(word)==1): \n",
    "                        unique.append(word)\n",
    "                        if word not in unique_word:\n",
    "                            unique_word.append(word)\n",
    "                df.loc[i,\"Tweets\"] = \",\".join(unique)\n",
    "                \n",
    "            self.true_count = dict(zip(unique_word , np.zeros(len(unique_word))))\n",
    "            \n",
    "            for i in range(len(df)):\n",
    "                for word in df.loc[i,\"Tweets\"].split(\",\"):\n",
    "                    self.true_count[word] += 1\n",
    "            \n",
    "            self.true_num = len(df)\n",
    "                \n",
    "            ##  ----------------------------  FALSE COUNT  ----------------------------  ##\n",
    "            \n",
    "            unique_word = []\n",
    "            \n",
    "            df = self.data[self.data['target(fake=0)']==0].reset_index(drop=True)\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                unique = []\n",
    "                text = df.loc[i,\"Tweets\"].split(\", \")\n",
    "                for word in text:\n",
    "                    if (text.count(word)>1 and (word not in unique) or text.count(word)==1): \n",
    "                        unique.append(word)\n",
    "                        if word not in unique_word:\n",
    "                            unique_word.append(word)\n",
    "                df.loc[i,\"Tweets\"] = \",\".join(unique)\n",
    "                \n",
    "            self.false_count = dict(zip(unique_word , np.zeros(len(unique_word))))\n",
    "\n",
    "            for i in range(len(df)):\n",
    "                for word in df.loc[i,\"Tweets\"].split(\",\"):\n",
    "                    self.false_count[word] += 1\n",
    "\n",
    "            self.false_num = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_Grams:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.ngram = data.reset_index(drop=True)\n",
    "        \n",
    "    def ngrams(self, n_grams):\n",
    "        \n",
    "        #### ------------------------------- UNIGRAMS ------------------------------- ####\n",
    "        \n",
    "        if ((1 == n_grams[0]) and (1 == n_grams[1])):\n",
    "            \n",
    "            for i in range(len(self.ngram)):\n",
    "                self.ngram.loc[i,\"Tweets\"] = \", \".join(self.ngram.loc[i,\"Tweets\"].split())\n",
    "        \n",
    "        #### ------------------------------- BIGRAMS ------------------------------- ####\n",
    "        \n",
    "        elif ((2 == n_grams[0]) and (2 == n_grams[1])):\n",
    "            \n",
    "            for i in range(len(self.ngram)):\n",
    "                bigram = []\n",
    "                text = self.ngram.loc[i,\"Tweets\"].split()\n",
    "                for j in range(len(text)-1):\n",
    "                    bigram.append(text[j]+\" \"+text[j+1])\n",
    "                self.ngram.loc[i,\"Tweets\"] = re.sub(r\"[\\[\\]]\" , \"\" , str(bigram))\n",
    "        \n",
    "        #### ------------------------------- UNIGRAMS + BIGRAMS ------------------------------- ####\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in range(len(self.ngram)):\n",
    "            \n",
    "                bigram = []\n",
    "                text = self.ngram.loc[i,\"Tweets\"].split()\n",
    "\n",
    "                for j in range(len(text)-1):\n",
    "                    bigram.append(text[j]+\" \"+text[j+1])\n",
    "\n",
    "                bigram = re.sub(r\"[\\[\\]]\" , \"\" , str(bigram))\n",
    "                unigram = re.sub(r\"[\\[\\]]\" , \"\" , str(self.ngram.loc[i,\"Tweets\"].split()))\n",
    "\n",
    "                self.ngram.loc[i,\"Tweets\"] = unigram + \", \" + bigram    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes(Preprocess , Word_Count, N_Grams):\n",
    "        \n",
    "    def __init__(self, prob_type='word', n_grams = (1,2)):\n",
    "        self.true_dict = []\n",
    "        self.false_dict = []\n",
    "        self.predicted = []\n",
    "        self.n_grams = n_grams\n",
    "        self.prob_type = prob_type\n",
    "        \n",
    "    def fit(self, train_data):    \n",
    "        \n",
    "        N_Grams.__init__(self, train_data)\n",
    "        self.ngrams(self.n_grams)            \n",
    "        Word_Count.__init__(self, self.ngram)\n",
    "        self.count(self.prob_type)\n",
    "           \n",
    "    def predict(self, test_data):\n",
    "        \n",
    "        Preprocess.__init__(self, test_data)\n",
    "        self.clean()\n",
    "\n",
    "        N_Grams.__init__(self, self.df)\n",
    "        self.ngrams(self.n_grams)\n",
    "        data = self.ngram.copy()\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            alpha = 0\n",
    "            true_count, false_count = 0 , 0\n",
    "            \n",
    "            self.true_dict = self.true_count.copy()\n",
    "            self.false_dict = self.false_count.copy()\n",
    "            \n",
    "            text = data.loc[i,\"Tweets\"].split(\", \")\n",
    "\n",
    "            for word in text:\n",
    "                if ((word not in self.true_dict.keys()) and (word not in self.false_dict.keys())):\n",
    "                    self.true_dict[word] , self.false_dict[word] = 0 , 0\n",
    "                    alpha = 1\n",
    "                    true_count += 1\n",
    "                    false_count += 1\n",
    "        \n",
    "                elif ((word not in self.true_dict.keys()) and (word in self.false_dict.keys())):\n",
    "                    self.true_dict[word] = 0\n",
    "                    alpha = 1\n",
    "                    true_count += 1\n",
    "                    \n",
    "                elif ((word in self.true_dict.keys()) and (word not in self.false_dict.keys())):\n",
    "                    self.false_dict[word] = 0\n",
    "                    alpha = 1\n",
    "                    false_count += 1\n",
    "\n",
    "            if self.prob_type == 'word':            \n",
    "                true_prob = (np.array(list(self.true_dict.values())) + alpha) / (self.true_num + ((len(self.true_dict) + true_count) * alpha) + 5)\n",
    "                false_prob = (np.array(list(self.false_dict.values())) + alpha) / (self.false_num + ((len(self.false_dict) + false_count) * alpha) + 5)\n",
    "            else:\n",
    "                true_prob = (np.array(list(self.true_dict.values())) + alpha) / (self.true_num + true_count + 5)\n",
    "                false_prob = (np.array(list(self.false_dict.values())) + alpha) / (self.false_num + false_count + 5)\n",
    "\n",
    "            true_prob = dict(zip(self.true_dict.keys() , true_prob))\n",
    "            false_prob = dict(zip(self.false_dict.keys() , false_prob))\n",
    "\n",
    "            True_Prob = len(self.true_dict) / (len(self.true_dict) + len(self.false_dict))\n",
    "            False_Prob = 1 - True_Prob\n",
    "\n",
    "            for word in text:\n",
    "                True_Prob *= true_prob[word]\n",
    "                False_Prob *= false_prob[word]\n",
    "\n",
    "            if (np.log(True_Prob) > np.log(False_Prob)):\n",
    "                self.predicted.append(1)\n",
    "            else:\n",
    "                self.predicted.append(0)\n",
    "                \n",
    "        return self.predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train , test = train_test_split(data , test_size = 200 , random_state=42 , stratify=data.iloc[:,-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Naive_Bayes(n_grams=(1,2), prob_type='word')\n",
    "clf.fit(train)\n",
    "pred = clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[105,  21],\n",
       "       [ 15,  59]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test.iloc[:,-1].values , pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       126\n",
      "           1       0.74      0.80      0.77        74\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.81      0.82      0.81       200\n",
      "weighted avg       0.82      0.82      0.82       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test.iloc[:,-1].values , pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
